# core/ml_detector.py

import os
import json
import numpy as np
import pandas as pd
import pefile
import asyncio
from typing import Dict, List, Tuple, Any
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
import joblib
from datetime import datetime
from utils.logging_utils import setup_logging
from utils.file_utils import create_safe_directory
from utils.hash_utils import calculate_hashes
from utils.string_utils import extract_iocs
from .base import AnalyzerBase

"""
This is an absolute mess right now, trying to find better libraries as sklearn is depreciated
"""

class MLDetector(AnalyzerBase):

    def __init__(self, config: Dict = None):
        super().__init__()
        self.logger = setup_logging('data/logs')
        self.model_path = config.get('model_path', 'data/models') if config else 'data/models'
        create_safe_directory(self.model_path)
        
        self.models = self._initialize_models()
        self.scaler = self._load_or_create_scaler('scaler.pkl')

    def _initialize_models(self) -> Dict:
        return {
            'random_forest': self._load_or_create_model('rf_model.pkl', RandomForestClassifier(
                n_estimators=200,
                max_depth=20,
                random_state=42,
                class_weight='balanced'
            )),
            'neural_net': self._load_or_create_model('nn_model.pkl', MLPClassifier(
                hidden_layer_sizes=(100, 50),
                max_iter=500,
                random_state=42
            )),
            'isolation_forest': self._load_or_create_model('if_model.pkl', IsolationForest(
                contamination=0.1,
                random_state=42
            ))
        }

    def _hash_to_features(self, hash_dict: Dict[str, str]) -> Dict[str, int]:
        features = {}
        for hash_type, hash_value in hash_dict.items():

            features[f"{hash_type}_sum"] = sum(ord(c) for c in hash_value)

            features[f"{hash_type}_zeros"] = hash_value.count('0')
            features[f"{hash_type}_ones"] = hash_value.count('1')
        return features

    def _extract_pe_features(self, sample_path: str) -> Dict:
        try:
            pe = pefile.PE(sample_path)
            return {
                'number_of_sections': len(pe.sections),
                'number_of_imports': len(pe.DIRECTORY_ENTRY_IMPORT) if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT') else 0,
                'number_of_exports': len(pe.DIRECTORY_ENTRY_EXPORT.symbols) if hasattr(pe, 'DIRECTORY_ENTRY_EXPORT') else 0,
                'has_debug': 1 if hasattr(pe, 'DIRECTORY_ENTRY_DEBUG') else 0,
                'has_tls': 1 if hasattr(pe, 'DIRECTORY_ENTRY_TLS') else 0,
                'has_resources': 1 if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE') else 0,
                'has_signature': 1 if hasattr(pe, 'DIRECTORY_ENTRY_SECURITY') else 0,
                'has_relocation': 1 if hasattr(pe, 'DIRECTORY_ENTRY_BASERELOC') else 0
            }
        except:
            return {
                'number_of_sections': 0,
                'number_of_imports': 0,
                'number_of_exports': 0,
                'has_debug': 0,
                'has_tls': 0,
                'has_resources': 0,
                'has_signature': 0,
                'has_relocation': 0
            }

    async def analyze(self, sample_path: str) -> Dict:
        try:
            self._start_analysis()
            self.logger.info(f"Starting ML analysis of {sample_path}")
            
            features = await self._extract_features(sample_path)
            feature_vector = self._prepare_features(features)
            
            results = {
                'predictions': self._get_predictions(feature_vector),
                'feature_importance': self._get_feature_importance(),
                'anomaly_score': self._get_anomaly_score(feature_vector),
                'confidence_scores': self._get_confidence_scores(feature_vector),
                'features': features,
                'metadata': self._add_metadata(),
                'timestamp': datetime.now().isoformat()
            }
            
            self._end_analysis()
            self.logger.info("ML analysis completed successfully")
            return results
        
        except Exception as e:
            self.logger.error(f"ML analysis failed: {str(e)}")
            return {'error': str(e)}

